{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_azure_openai_keys\n",
    "\n",
    "AZURE_API_KEY, AZURE_ENDPOINT, AZURE_API_VERSION = get_azure_openai_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def mystery(x: int, y: int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from helper import get_azure_llm\n",
    "\n",
    "llm = get_azure_llm()\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool],\n",
    "    \"Tell me the output of the mystery function on 2 and 9\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto retrieval tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_azure_llm, get_azure_embed_model\n",
    "\n",
    "llm = get_azure_llm()\n",
    "embed_model = get_azure_embed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"../data/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 26\n",
      "file_name: metagpt.pdf\n",
      "file_path: ../data/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2025-01-02\n",
      "last_modified_date: 2025-01-02\n",
      "\n",
      "Preprint\n",
      "Figure 11: The system interface design for “recommendation engine development” is generated by\n",
      "the architect agent (zoom in for a better view).\n",
      "E M ORE DISCUSSIONS\n",
      "E.1 D EEP -SEATED CHALLENGES\n",
      "MetaGPT also alleviates or solves these challenges with its unique designs:\n",
      "Use Context Efficiently Two sub-challenges are present. First, unfolding short natural language\n",
      "descriptions accurately to eliminate ambiguity. Second, maintaining information validity in lengthy\n",
      "contexts, enables LLMs to concentrate on relevant data without distraction.\n",
      "Reduce Hallucinations Using LLMs to generate entire software programs faces code halluci-\n",
      "nation problems—-including incomplete implementation of functions, missing dependencies, and\n",
      "potential undiscovered bugs, which may be more serious. LLMs often struggle with software gen-\n",
      "eration due to vague task definitions. Focusing on granular tasks like requirement analysis and\n",
      "package selection offers guided thinking, which LLMs lack in broad task solving.\n",
      "E.2 I NFORMATION OVERLOAD\n",
      "In MetaGPT, we use a global message pool and a subscription mechanism to address “information\n",
      "overload,” which refers to the problem of receiving excessive or irrelevant information. This issue\n",
      "is dependent on specific applications. MetaGPT employs a message pool to streamline communi-\n",
      "cation, ensuring efficiency. Additionally, a subscription mechanism filters out irrelevant contexts,\n",
      "enhancing the relevance and utility of the information. This design is particularly crucial in soft-\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(nodes[30].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 page_label: 1\n",
      "\n",
      "1 page_label: 2\n",
      "\n",
      "2 page_label: 3\n",
      "\n",
      "3 page_label: 3\n",
      "\n",
      "4 page_label: 4\n",
      "\n",
      "5 page_label: 5\n",
      "\n",
      "6 page_label: 6\n",
      "\n",
      "7 page_label: 7\n",
      "\n",
      "8 page_label: 7\n",
      "\n",
      "9 page_label: 8\n",
      "\n",
      "10 page_label: 9\n",
      "\n",
      "11 page_label: 10\n",
      "12 page_label: 10\n",
      "13 page_label: 11\n",
      "14 page_label: 11\n",
      "15 page_label: 12\n",
      "16 page_label: 12\n",
      "17 page_label: 13\n",
      "18 page_label: 14\n",
      "19 page_label: 15\n",
      "20 page_label: 16\n",
      "21 page_label: 17\n",
      "22 page_label: 18\n",
      "23 page_label: 19\n",
      "24 page_label: 20\n",
      "25 page_label: 21\n",
      "26 page_label: 22\n",
      "27 page_label: 23\n",
      "28 page_label: 24\n",
      "29 page_label: 25\n",
      "30 page_label: 26\n",
      "31 page_label: 27\n",
      "32 page_label: 28\n",
      "33 page_label: 29\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    print(f\"{i} {node.get_metadata_str(mode='embed')[:14]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts([{\"key\": \"page_label\", \"value\": \"2\"}]),\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 for code generation benchmarks. It also demonstrates a 100% task completion rate, showcasing its robustness and efficiency in handling complex software projects.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '../data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-02', 'last_modified_date': '2025-01-02'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define auto retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(query: str, page_numbers: List[str]) -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "\n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [{\"key\": \"page_label\", \"value\": p} for p in page_numbers]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts, condition=FilterCondition.OR\n",
    "        ),\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(name=\"vector_tool\", fn=vector_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 on code generation benchmarks. It also demonstrates a 100% task completion rate, showcasing its robustness and efficiency in handling complex software projects.\n"
     ]
    }
   ],
   "source": [
    "llm = get_azure_llm()\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '../data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-02', 'last_modified_date': '2025-01-02'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and with other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\"Useful if you want to get a summary of MetaGPT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev in several aspects on the SoftwareDev dataset. MetaGPT achieves a higher executability score of 3.75 compared to ChatDev's 2.25. It also takes less time to run (503 seconds versus 762 seconds) and requires fewer tokens to generate one line of code (126.5/124.3 tokens compared to ChatDev's 248.9 tokens). Additionally, MetaGPT produces more code files and lines of code per file, resulting in a higher total number of code lines. The cost of human revision is also significantly lower for MetaGPT (0.83) compared to ChatDev (2.5).\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': '../data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-02', 'last_modified_date': '2025-01-02'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"MetaGPT\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is a meta-programming framework designed to enhance multi-agent collaboration in software development using large language models (LLMs). It employs a structured division of labor among specialized agents, such as Product Managers, Architects, Engineers, and QA Engineers, to streamline workflows and reduce errors in complex tasks. MetaGPT follows Standardized Operating Procedures (SOPs) and uses documents and diagrams for communication, rather than dialogue, to ensure clarity and consistency. The framework incorporates mechanisms for self-improvement and iterative feedback to improve code quality and productivity. It has demonstrated superior performance in benchmarks like HumanEval and MBPP, showcasing its robustness and efficiency in handling complex software engineering projects. MetaGPT also supports user data privacy by operating locally and not collecting user data.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \"What is a summary of the paper?\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
